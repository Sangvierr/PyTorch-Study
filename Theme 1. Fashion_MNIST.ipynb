{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2024.01.28] ì´ìƒí˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°œìš”\n",
    "- [ì¶œì²˜] ë”¥ëŸ¬ë‹ íŒŒì´í† ì¹˜ êµê³¼ì„œ 5ì¥ 2ì ˆì˜ í•©ì„±ê³± ì‹ ê²½ë§ ë§›ë³´ê¸°\n",
    "- fashion_mnist ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í•©ì„±ê³± ì‹ ê²½ë§ì„ ì§ì ‘ êµ¬í˜„í•´ë³´ì.\n",
    "- fashion_mnist ë°ì´í„°ì…‹ì€ í† ì¹˜ë¹„ì „ì— ë‚´ì¥ëœ ì˜ˆì œ ë°ì´í„°ë¡œ ìš´ë™í™”, ì…”ì¸ , ìƒŒë“¤ ê°™ì€ ì‘ì€ ì´ë¯¸ì§€ì˜ ëª¨ìŒì´ë©°, ê¸°ë³¸ MNIST ë°ì´í„°ì…‹ì²˜ëŸ¼ ì—´ ê°€ì§€ë¡œ ë¶„ë¥˜ë  ìˆ˜ ìˆëŠ” 28X28 í”½ì…€ì˜ ì´ë¯¸ì§€ 7ë§Œ ê°œë¡œ ë˜ì–´ ìˆë‹¤.\n",
    "- í›ˆë ¨ ë°ì´í„°ëŠ” 0ë¶€í„° 255 ì‚¬ì´ì˜ ê°’ì„ ê°–ëŠ” 28X28 í¬ê¸°ì˜ ë„˜íŒŒì´ ë°°ì—´ì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. GPU Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’»ë¹ ë¥¸ ì—°ì‚°ì„ ìœ„í•´ Tensorflowì—ì„œ **GPU**ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•´ì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU ì„¤ì •ì´ ë§ˆë¬´ë¦¬ ë˜ì—ˆë‹¤. ì´ì œ ì‹œë“œë¥¼ ì„¤ì •í•˜ê³ ì„œ ë³¸ê²©ì ìœ¼ë¡œ ì‹œì‘í•´ë³´ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œë“œ ê³ ì •\n",
    "torch.manual_seed(128)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms # data pre-processing\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì. í† ì¹˜ë¹„ì „ìœ¼ë¡œ ë‹¤ìš´ ë°›ì„ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST('C:/Python_Programs/Pytorch/1. Fashion MNIST', download=True,\n",
    "                                               transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_data =  torchvision.datasets.FashionMNIST('C:/Python_Programs/Pytorch/1. Fashion MNIST', download=True, train=False,\n",
    "                                               transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìš´ ë°›ì€ ë°ì´í„°ë¥¼ ë§¤ëª¨ë¦¬ë¡œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ ë°ì´í„°ë¡œë”(DataLoader)ì— ì „ë‹¬í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=100)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size=100ìœ¼ë¡œ ì§€ì •í•´ì„œ 100ê°œ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¬¶ì–´ì„œ ë¶ˆëŸ¬ì™”ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë²ˆì—ëŠ” ìš°ë¦¬ê°€ ë‹¤ë£¨ê²Œ ë  ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ì‚´í´ë³´ì. ë¨¼ì € 20ê°œ ì •ë„ì˜ ì´ë¯¸ì§€ë¥¼ ë ˆì´ë¸” ì •ë³´ì™€ í•¨ê»˜ ì¶œë ¥í•´ë³´ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "labels_map = {0 : 'T-Shirt', 1 : 'Trouser', 2 : 'Pullover', 3 : 'Dress', 4 : 'Coat', \n",
    "              5 : 'Sandal', 6 : 'Shirt', 7 : 'Sneaker', 8 : 'Bag', 9 : 'Ankle Boot'}\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "columns = 4\n",
    "rows = 5\n",
    "\n",
    "for i in range(1, columns*rows+1):\n",
    "    img_xy = np.random.randint(len(train_loader)) # randomly choose 1 integer\n",
    "    img = train_data[img_xy][0][0,:,:] # randomly choose 1 image\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.title(labels_map[train_data[img_xy][1]]) # title with labels_map\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1. Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¨¼ì € ì¼ë°˜ì ì¸ ì‹¬ì¸µ ì‹¬ê²½ë§ì„ ìƒì„±í•´ì„œ í•™ìŠµì‹œì¼œë³´ì. ë¨¼ì € í´ë˜ìŠ¤ë¡œ ì‹ ê²½ë§ì„ êµ¬í˜„í•´ë³´ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=784, out_features=256)\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        out = input_data.view(-1, 784)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.drop(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹ ê²½ë§ì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "1. ì„ í˜•íšŒê·€ : 784 í¬ê¸°ì˜ ì…ë ¥ì„ ë°›ìœ¼ë©´, 256 í¬ê¸°ë¡œ ì¶œë ¥í•œë‹¤.\n",
    "2. ë“œë¡­ì•„ì›ƒ : ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ê³„ì¸µìœ¼ë¡œ, 25%ì˜ ë‰´ëŸ°ì„ 0ìœ¼ë¡œ ë§Œë“¤ê³  75%ì— í•´ë‹¹í•˜ëŠ” ë‰´ëŸ°ì€ (1/(1-0.7))ë§Œí¼ í‚¤ìš´ë‹¤.\n",
    "3. ì„ í˜•íšŒê·€ : 256 í¬ê¸°ì˜ ì…ë ¥ì„ ë°›ìœ¼ë©´, 128 í¬ê¸°ë¡œ ì¶œë ¥í•œë‹¤.\n",
    "4. ì„ í˜•íšŒê·€ : 128 í¬ê¸°ì˜ ì…ë ¥ì„ ë°›ìœ¼ë©´, 10 í¬ê¸°ë¡œ ì¶œë ¥í•œë‹¤. ì´ëŠ” 10ê°œì˜ ë ˆì´ë¸”ë¡œ êµ¬ë¶„í•˜ê¸° ìœ„í•¨ì´ë‹¤.\n",
    "\n",
    "\n",
    "ìˆœì „íŒŒì˜ ê²½ìš°ëŠ” ë‹¤ìŒ ê³¼ì •ì„ ë”°ë¥¸ë‹¤.\n",
    "\n",
    "1. view : ë‘ë²ˆì§¸ ì°¨ì›ì„ 784ë¡œ ê³ ì • \n",
    "2. ì²«ë²ˆì§¸ ì„ í˜•íšŒê·€ ì¸µ í†µê³¼ -> ë ë£¨ í•¨ìˆ˜ í†µê³¼\n",
    "3. ë“œë¡­ì•„ì›ƒ ê³„ì¸µ í†µê³¼\n",
    "4. ë‘ë²ˆì§¸ ì„ í˜•íšŒê·€ ì¸µ í†µê³¼ -> ë ë£¨ í•¨ìˆ˜ í†µê³¼ \n",
    "5. ì„¸ë²ˆì§¸ ì„ í˜•íšŒê·€ì¸µ í†µê³¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš  ì´ë•Œ ë ë£¨ í•¨ìˆ˜ì—ëŠ” ë‘ ê°€ì§€ ì‚¬ìš©ë²•ì´ ìˆë‹¤.\n",
    "- torch.nn.functional.relu() : ìˆœì „íŒŒ ë©”ì„œë“œì¸ forward()ì—ì„œ ì‚¬ìš©\n",
    "- torch.nn.ReLU() : ì´ˆê¸°í™” ë©”ì„œë“œì¸ init()ì—ì„œ ì‚¬ìš©, nn.Sequential() ì•ˆì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ nn.ReLU()ë¥¼ ì‚¬ìš©í•˜ë©´ ë¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ í•™ìŠµì„ ìœ„í•œ ì†ì‹¤í•¨ìˆ˜, í•™ìŠµë¥ , ì˜µí‹°ë§ˆì´ì €ë¥¼ ì •ì˜í•˜ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionDNN(\n",
       "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (drop): Dropout(p=0.25, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model = FashionDNN()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ í•™ìŠµì„ ì§„í–‰í•˜ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:27<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.41, Accuracy: 83.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:27<00:00, 21.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.35, Accuracy: 85.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:27<00:00, 21.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.23, Accuracy: 85.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:27<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.31, Accuracy: 86.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:27<00:00, 21.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.27, Accuracy: 86.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "        train = Variable(images.view(100, 1, 28, 28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        outputs = model(train)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "    \n",
    "        if (count % 50) == 0:    \n",
    "            total = 0\n",
    "            correct = 0        \n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                labels_list.append(labels)            \n",
    "                test = Variable(images.view(100, 1, 28, 28))            \n",
    "                outputs = model(test)            \n",
    "                predictions = torch.max(outputs, 1)[1].to(device)\n",
    "                predictions_list.append(predictions)\n",
    "                correct += (predictions == labels).sum()            \n",
    "                total += len(labels)\n",
    "            \n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        \n",
    "    # ë§¤ epochì˜ ëì—ì„œ ì†ì‹¤ê³¼ ì •í™•ë„ ì¶œë ¥\n",
    "    tqdm.write(\"Epoch: {}, Loss: {:.2f}, Accuracy: {:.2f}%\".format(epoch+1, loss.data, accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tqdmì„ ì‚¬ìš©í•´ì„œ í•™ìŠµ ì¶”ì´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤.\n",
    "- ì •í™•ë„ëŠ” ì•½ 86% ì •ë„ë¡œ ë‚˜ì˜ì§€ ì•Šë‹¤ê³  ë³¸ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë²ˆì—ëŠ” ì´ë¯¸ì§€ ë°ì´í„°ì— íŠ¹í™”ëœ CNNì„ ëª¨ë¸ë¡œ ë§Œë“¤ì–´ë³´ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(FashionCNN, self).__init__()        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )       \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )        \n",
    "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)       \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•©ì„±ê³± ì‹ ê²½ë§ê³¼ ì „ê²°í•©ì¸µì— ëŒ€í•´ì„œ ìì„¸íˆ ì•Œì•„ë³´ì.\n",
    "1. í•©ì„±ê³±ì¸µ : í•„í„° ë†’ì´ $\\times$ ë„ˆë¹„ í¬ê¸°ì˜ ì´ë¯¸ì§€ë¥¼ ì²˜ìŒë¶€í„° ëê¹Œì§€ í›‘ìœ¼ë©´ì„œ ê°’ì„ ì¶œë ¥í•œë‹¤. ì—¬ê¸°ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ê¸ˆë§Œ ë” ì‚´í´ë³´ì.\n",
    "- in_channels : ì…ë ¥ ì±„ë„ì˜ ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. í‘ë°±ì¼ ë•ŒëŠ” 1, RGBì¼ ë•ŒëŠ” 3ìœ¼ë¡œ ì„¤ì •í•œë‹¤. ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ì´ë¯¸ì§€ ë°ì´í„°ëŠ” í‘ë°±ì´ê¸° ë•Œë¬¸ì— 1ë¡œ ì„¤ì •\n",
    "- out_channels : ì¶œë ¥ ì±„ë„ì˜ ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.\n",
    "- kernel_size : ì»¤ë„ì˜ í¬ê¸°ì´ë©°, ì»¤ë„ì˜ íŒŒë¼ë¯¸í„°ê°€ CNNì˜ í•™ìŠµ ëŒ€ìƒì´ë‹¤.\n",
    "- padding : íŒ¨ë”© í¬ê¸°ë¥¼ ì˜ë¯¸í•˜ë©°, í•©ì„±ê³± ì—°ì„ ì„ ìˆ˜í–‰í•˜ê¸° ì´ì „ì— ì…ë ¥ ë°ì´í„°ì˜ í¬ê¸°ë¥¼ í‚¤ìš´ë‹¤.\n",
    "\n",
    "2. ë°°ì¹˜ ì •ê·œí™” ê³„ì¸µ : ë°ì´í„°ì˜ ë¶„í¬ë¥¼ í‰ê· ê³¼ ë¶„ì‚°ì„ ì´ìš©í•˜ì—¬ ì •ê·œí™”í•˜ëŠ” ê²ƒì´ë‹¤.\n",
    "\n",
    "3. ë§¥ìŠ¤ í’€ë§ê³„ì¸µ : ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì¶•ì†Œì‹œí‚¤ëŠ” ìš©ë„ë¡œ ì‚¬ìš©í•œë‹¤. íŒŒë¼ë¯¸í„°ë¥¼ ì‚´í´ë³´ì\n",
    "- kernel_size : ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì˜ë¯¸í•œë‹¤.\n",
    "- stride : ì´ë™ ê°„ê²©\n",
    "\n",
    "4. ì„ í˜• íšŒê·€ ê³„ì¸µ : ì´ë¯¸ì§€ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë°°ì—´ í˜•íƒœë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤. \n",
    "- in_features : íŒ¨ë”©, ìŠ¤íŠ¸ë¼ì´ë“œ ë“±ì„ ì ìš©í–ˆê¸° ë•Œë¬¸ì— ì´ì— ë”°ë¥¸ ì¶œë ¥ ì°¨ì›ì„ ê³„ì‚°í•´ì•¼ í•œë‹¤.\n",
    "- ìˆœì „íŒŒì—ì„œ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ out.view(out.size(0), -1)ì„ ì‚¬ìš©í•œë‹¤. out.size(0)ì€ ê²°êµ­ 100ì„ ì˜ë¯¸í•œë‹¤.\n",
    "\n",
    "5. ë“œë¡­ì•„ì›ƒ\n",
    "\n",
    "6. ì„ í˜• íšŒê·€\n",
    "\n",
    "7. ì„ í˜• íšŒê·€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ í•™ìŠµì„ ì§„í–‰í•´ë³´ì. ì½”ë“œëŠ” ì´ì „ê³¼ ë˜‘ê°™ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionCNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=2304, out_features=600, bias=True)\n",
      "  (drop): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=600, out_features=120, bias=True)\n",
      "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# model\n",
    "model = FashionCNN()\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:38<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.28, Accuracy: 87.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:38<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.25, Accuracy: 86.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:39<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.17, Accuracy: 88.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:36<00:00, 16.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.15, Accuracy: 90.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:36<00:00, 16.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.16, Accuracy: 90.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "        train = Variable(images.view(100, 1, 28, 28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        outputs = model(train)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "    \n",
    "        if (count % 50) == 0:    \n",
    "            total = 0\n",
    "            correct = 0        \n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                labels_list.append(labels)            \n",
    "                test = Variable(images.view(100, 1, 28, 28))            \n",
    "                outputs = model(test)            \n",
    "                predictions = torch.max(outputs, 1)[1].to(device)\n",
    "                predictions_list.append(predictions)\n",
    "                correct += (predictions == labels).sum()            \n",
    "                total += len(labels)\n",
    "            \n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        \n",
    "    # ë§¤ epochì˜ ëì—ì„œ ì†ì‹¤ê³¼ ì •í™•ë„ ì¶œë ¥\n",
    "    tqdm.write(\"Epoch: {}, Loss: {:.2f}, Accuracy: {:.2f}%\".format(epoch+1, loss.data, accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì •í™•ë„ê°€ ë¬´ë ¤ 90%ë‚˜ ë‚˜ì˜¤ê³  ìˆë‹¤.\n",
    "- ê¸°ë³¸ ì‹¬ì¸µ ì‹ ê²½ë§ë³´ë‹¤ëŠ” ì•½ 4% ì •ë„ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
